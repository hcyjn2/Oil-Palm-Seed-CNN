{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PxpNpH3GXIcF"
      },
      "source": [
        "Run section below if using Google Colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LujrziSNXF4T",
        "outputId": "7f9861a3-4d15-4886-c9d7-e547381b7016"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-0.9.2-py3-none-any.whl (419 kB)\n",
            "\u001b[K     |████████████████████████████████| 419 kB 26.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (21.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (4.1.1)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (1.21.6)\n",
            "Requirement already satisfied: torch>=1.3.1 in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (1.12.0+cu113)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->torchmetrics) (3.0.9)\n",
            "Installing collected packages: torchmetrics\n",
            "Successfully installed torchmetrics-0.9.2\n",
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# import library\n",
        "!pip install torchmetrics\n",
        "\n",
        "# mount drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import os\n",
        "os.chdir('/content/drive/MyDrive/COMP3029 - CV/Oil-Palm-Seed-CNN-master')\n",
        "\n",
        "# using Tensorboard on Google Colab \n",
        "%load_ext tensorboard\n",
        "# %tensorboard --logdir logs # run this code to display Tensorboard panel on Google Colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mw0pfYZ1-uUX"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision.transforms import ToTensor, Lambda\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import cm\n",
        "import cv2 as cv\n",
        "from zipfile import ZipFile\n",
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "from torchvision.io import read_image\n",
        "from PIL import Image\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torchmetrics import F1Score\n",
        "from itertools import permutations\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "\n",
        "import torch.nn as nn \n",
        "import torch.nn.functional as F \n",
        "\n",
        "trained_weight = False\n",
        "validation_in_training = True\n",
        "writer = SummaryWriter('RGB_New_run/')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-l29A6LJXE__"
      },
      "source": [
        "# Data Prep"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xQ5UsoIyOWjH",
        "outputId": "db3b801d-6099-4f8a-c294-4ee158e8d38a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training set 159\n",
            "test set 46\n"
          ]
        }
      ],
      "source": [
        "train_x = []\n",
        "train_y = []\n",
        "test_x = []\n",
        "test_y = []\n",
        "\n",
        "# Testing Dataset\n",
        "for idx, i in enumerate(os.listdir(\"test/good_seed/\")):\n",
        "    view1 = cv.cvtColor(cv.resize(cv.imread('test/good_seed/%s/%s_x1.jpg' % (i,i)), (64,64), interpolation = cv.INTER_AREA), cv.COLOR_BGR2RGB)\n",
        "    view2 = cv.cvtColor(cv.resize(cv.imread('test/good_seed/%s/%s_x2.jpg' % (i,i)), (64,64), interpolation = cv.INTER_AREA), cv.COLOR_BGR2RGB)\n",
        "    view3 = cv.cvtColor(cv.resize(cv.imread('test/good_seed/%s/%s_x3.jpg' % (i,i)), (64,64), interpolation = cv.INTER_AREA), cv.COLOR_BGR2RGB)\n",
        "    view4 = cv.cvtColor(cv.resize(cv.imread('test/good_seed/%s/%s_x4.jpg' % (i,i)), (64,64), interpolation = cv.INTER_AREA), cv.COLOR_BGR2RGB)\n",
        "    view5 = cv.cvtColor(cv.resize(cv.imread('test/good_seed/%s/%s_x5.jpg' % (i,i)), (64,64), interpolation = cv.INTER_AREA), cv.COLOR_BGR2RGB)\n",
        "\n",
        "    x = [view1, view2, view3, view4, view5]\n",
        "    y = 1 # Class Good Seed \n",
        "    test_x.append(x)\n",
        "    test_y.append(y)\n",
        "\n",
        "for idx, i in enumerate(os.listdir(\"test/bad_seed/\")):\n",
        "    view1 = cv.cvtColor(cv.resize(cv.imread('test/bad_seed/%s/%s_x1.jpg' % (i,i)), (64,64), interpolation = cv.INTER_AREA), cv.COLOR_BGR2RGB)\n",
        "    view2 = cv.cvtColor(cv.resize(cv.imread('test/bad_seed/%s/%s_x2.jpg' % (i,i)), (64,64), interpolation = cv.INTER_AREA), cv.COLOR_BGR2RGB)\n",
        "    view3 = cv.cvtColor(cv.resize(cv.imread('test/bad_seed/%s/%s_x3.jpg' % (i,i)), (64,64), interpolation = cv.INTER_AREA), cv.COLOR_BGR2RGB)\n",
        "    view4 = cv.cvtColor(cv.resize(cv.imread('test/bad_seed/%s/%s_x4.jpg' % (i,i)), (64,64), interpolation = cv.INTER_AREA), cv.COLOR_BGR2RGB)\n",
        "    view5 = cv.cvtColor(cv.resize(cv.imread('test/bad_seed/%s/%s_x5.jpg' % (i,i)), (64,64), interpolation = cv.INTER_AREA), cv.COLOR_BGR2RGB)\n",
        "\n",
        "    x = [view1, view2, view3, view4, view5]\n",
        "    y = 0 # Class Bad Seed \n",
        "    test_x.append(x)\n",
        "    test_y.append(y)\n",
        "\n",
        "test_x = np.array(test_x)\n",
        "test_y = np.array(test_y)\n",
        "test_tensors_x = torch.Tensor(test_x)\n",
        "test_tensors_y = torch.Tensor(test_y)\n",
        "\n",
        "test_dataset = TensorDataset(test_tensors_x, test_tensors_y)\n",
        "\n",
        "\n",
        "# Training Dataset\n",
        "for idx, i in enumerate(os.listdir(\"train/good_seed/\")):\n",
        "    view1 = cv.cvtColor(cv.resize(cv.imread('train/good_seed/%s/%s_x1.jpg' % (i,i)), (64,64), interpolation = cv.INTER_AREA), cv.COLOR_BGR2RGB)\n",
        "    view2 = cv.cvtColor(cv.resize(cv.imread('train/good_seed/%s/%s_x2.jpg' % (i,i)), (64,64), interpolation = cv.INTER_AREA), cv.COLOR_BGR2RGB)\n",
        "    view3 = cv.cvtColor(cv.resize(cv.imread('train/good_seed/%s/%s_x3.jpg' % (i,i)), (64,64), interpolation = cv.INTER_AREA), cv.COLOR_BGR2RGB)\n",
        "    view4 = cv.cvtColor(cv.resize(cv.imread('train/good_seed/%s/%s_x4.jpg' % (i,i)), (64,64), interpolation = cv.INTER_AREA), cv.COLOR_BGR2RGB)\n",
        "    view5 = cv.cvtColor(cv.resize(cv.imread('train/good_seed/%s/%s_x5.jpg' % (i,i)), (64,64), interpolation = cv.INTER_AREA), cv.COLOR_BGR2RGB)\n",
        "\n",
        "    x = [view1, view2, view3, view4, view5]\n",
        "    y = 1 # Class Bad Seed \n",
        "    train_x.append(x)\n",
        "    train_y.append(y)\n",
        "\n",
        "for idx, i in enumerate(os.listdir(\"train/bad_seed/\")):\n",
        "    view1 = cv.cvtColor(cv.resize(cv.imread('train/bad_seed/%s/%s_x1.jpg' % (i,i)), (64,64), interpolation = cv.INTER_AREA), cv.COLOR_BGR2RGB)\n",
        "    view2 = cv.cvtColor(cv.resize(cv.imread('train/bad_seed/%s/%s_x2.jpg' % (i,i)), (64,64), interpolation = cv.INTER_AREA), cv.COLOR_BGR2RGB)\n",
        "    view3 = cv.cvtColor(cv.resize(cv.imread('train/bad_seed/%s/%s_x3.jpg' % (i,i)), (64,64), interpolation = cv.INTER_AREA), cv.COLOR_BGR2RGB)\n",
        "    view4 = cv.cvtColor(cv.resize(cv.imread('train/bad_seed/%s/%s_x4.jpg' % (i,i)), (64,64), interpolation = cv.INTER_AREA), cv.COLOR_BGR2RGB)\n",
        "    view5 = cv.cvtColor(cv.resize(cv.imread('train/bad_seed/%s/%s_x5.jpg' % (i,i)), (64,64), interpolation = cv.INTER_AREA), cv.COLOR_BGR2RGB)\n",
        "\n",
        "    x = [view1, view2, view3, view4, view5]\n",
        "    y = 0 # Class Bad Seed \n",
        "    train_x.append(x)\n",
        "    train_y.append(y)\n",
        "\n",
        "train_x = np.array(train_x)\n",
        "train_y = np.array(train_y)\n",
        "train_tensors_x = torch.Tensor(train_x)\n",
        "train_tensors_y = torch.Tensor(train_y)\n",
        "\n",
        "train_dataset = TensorDataset(train_tensors_x, train_tensors_y)\n",
        "\n",
        "\n",
        "# ( Bad Seed = 78 + Good Seed 81 ) * Amount of Views Permutation 120 = 19080\n",
        "print('training set', len(train_dataset))\n",
        "\n",
        "# ( Bad Seed = 28 + Good Seed 18 ) * Amount of Views Permutation 120 = 5520\n",
        "print('test set', len(test_dataset))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8MkDrJFEirpv"
      },
      "outputs": [],
      "source": [
        "test_dataloader = DataLoader(test_dataset, batch_size=8, shuffle=True, num_workers=2)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gOSjEZtjXFAH"
      },
      "source": [
        "# CNN Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jJMTRUAR5uiY"
      },
      "outputs": [],
      "source": [
        "class FeatureNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.network = nn.Sequential(\n",
        "                    nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
        "                    nn.ReLU(),\n",
        "                    nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
        "                    nn.ReLU(),\n",
        "                    nn.MaxPool2d(2, 2), # output: 64 x 16 x 16\n",
        "                    nn.BatchNorm2d(64),\n",
        "\n",
        "                    nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
        "                    nn.ReLU(),\n",
        "                    nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n",
        "                    nn.ReLU(),\n",
        "                    nn.MaxPool2d(2, 2), # output: 128 x 8 x 8\n",
        "                    nn.BatchNorm2d(128),\n",
        "\n",
        "                    nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
        "                    nn.ReLU(),\n",
        "                    nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
        "                    nn.ReLU(),\n",
        "                    nn.MaxPool2d(2, 2), # output: 256 x 4 x 4\n",
        "                    nn.BatchNorm2d(256))\n",
        "\n",
        "    def forward(self, xb):\n",
        "        return self.network(xb)\n",
        "\n",
        "\n",
        "class ClassificationNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.network = nn.Sequential(\n",
        "                    nn.Linear(16384, 1024),   # change to from [5x256x8x8] to [256x8x8]\n",
        "                    nn.ReLU(),\n",
        "                    nn.Linear(1024, 2))\n",
        "\n",
        "    def forward(self, xb):\n",
        "        return self.network(xb)\n",
        "\n",
        "\n",
        "\n",
        "f1_net = FeatureNet()\n",
        "f2_net = FeatureNet()\n",
        "f3_net = FeatureNet()\n",
        "f4_net = FeatureNet()\n",
        "f5_net = FeatureNet()\n",
        "\n",
        "model = ClassificationNet()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gv00Cfi1WXF7",
        "outputId": "7644e82e-c951-4205-9a92-d343678bbb75"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu device\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FeatureNet(\n",
              "  (network): Sequential(\n",
              "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU()\n",
              "    (2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (3): ReLU()\n",
              "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (7): ReLU()\n",
              "    (8): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (9): ReLU()\n",
              "    (10): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (12): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (13): ReLU()\n",
              "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (15): ReLU()\n",
              "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (17): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using {} device'.format(device))\n",
        "model.to(device)\n",
        "f1_net.to(device)\n",
        "f2_net.to(device)\n",
        "f3_net.to(device)\n",
        "f4_net.to(device)\n",
        "f5_net.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gxHAA2VAlgQz"
      },
      "outputs": [],
      "source": [
        "def initialize_weights(model):\n",
        "  if isinstance(model, nn.Conv2d):\n",
        "      nn.init.kaiming_uniform_(model.weight.data,nonlinearity='relu')\n",
        "      if model.bias is not None:\n",
        "          nn.init.constant_(model.bias.data, 0)\n",
        "  elif isinstance(model, nn.BatchNorm2d):\n",
        "      nn.init.constant_(model.weight.data, 1)\n",
        "      nn.init.constant_(model.bias.data, 0)\n",
        "  elif isinstance(model, nn.Linear):\n",
        "      nn.init.kaiming_uniform_(model.weight.data)\n",
        "      nn.init.constant_(model.bias.data, 0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j-LFdilLlj6A"
      },
      "outputs": [],
      "source": [
        "model.apply(initialize_weights)\n",
        "f1_net.apply(initialize_weights)\n",
        "f2_net.apply(initialize_weights)\n",
        "f3_net.apply(initialize_weights)\n",
        "f4_net.apply(initialize_weights)\n",
        "f5_net.apply(initialize_weights)\n",
        "\n",
        "if trained_weight:\n",
        "  checkpoint = torch.load('grayscale.pt')\n",
        "  model.load_state_dict(checkpoint['model_state_dict'])\n",
        "  print('Weights Loaded')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MfASjlx9lkg6"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer_main = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "optimizer_f1 = optim.SGD(f1_net.parameters(), lr=0.001, momentum=0.9)\n",
        "optimizer_f2 = optim.SGD(f2_net.parameters(), lr=0.001, momentum=0.9)\n",
        "optimizer_f3 = optim.SGD(f3_net.parameters(), lr=0.001, momentum=0.9)\n",
        "optimizer_f4 = optim.SGD(f4_net.parameters(), lr=0.001, momentum=0.9)\n",
        "optimizer_f5 = optim.SGD(f5_net.parameters(), lr=0.001, momentum=0.9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74YmDWbTXFAM"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GjkAGBvalt-6",
        "outputId": "0ebd4554-649b-46a0-a765-b9a8d236fb99"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0/299\n",
            "----------\n",
            "Training Loss: 0.52627247\n",
            "Epoch 1/299\n",
            "----------\n",
            "Training Loss: 0.09062186\n",
            "Epoch 2/299\n",
            "----------\n",
            "Training Loss: 0.09399784\n",
            "Epoch 3/299\n",
            "----------\n",
            "Training Loss: 0.09018452\n",
            "Epoch 4/299\n",
            "----------\n",
            "Training Loss: 0.09619877\n",
            "Epoch 5/299\n",
            "----------\n",
            "Training Loss: 0.08637184\n",
            "Epoch 6/299\n",
            "----------\n",
            "Training Loss: 0.08909605\n",
            "Epoch 7/299\n",
            "----------\n",
            "Training Loss: 0.08665008\n",
            "Epoch 8/299\n",
            "----------\n",
            "Training Loss: 0.08798388\n",
            "Epoch 9/299\n",
            "----------\n",
            "Training Loss: 0.08733927\n",
            "Epoch 10/299\n",
            "----------\n",
            "Training Loss: 0.08775808\n",
            "Epoch 11/299\n",
            "----------\n"
          ]
        }
      ],
      "source": [
        "if trained_weight == False:\n",
        "\n",
        "  # Training with Validation \n",
        "  n_epochs = 300\n",
        "  last25 = 0\n",
        "  data_loaders = {'train': train_dataloader}\n",
        "  data_lengths = {'train': len(train_dataset)}\n",
        "  feature_map_shape = [8, 256, 8, 8]\n",
        "\n",
        "  for epoch in range(n_epochs):\n",
        "      print('Epoch {}/{}'.format(epoch, n_epochs - 1))\n",
        "      print('-' * 10)\n",
        "      model.train(True)  \n",
        "      running_loss = 0.0\n",
        "\n",
        "          # Iterate over data.\n",
        "      for images, labels in train_dataloader:\n",
        "              # wrap them in a torch Variable\n",
        "              images, labels = Variable(images), Variable(labels)\n",
        "\n",
        "              # torch.Size([8, 5, 64, 64, 3]) --> torch.Size([5, 8, 3, 64, 64])\n",
        "              images = images.view(images.shape[1], images.shape[0], images.shape[4], images.shape[2],images.shape[3])\n",
        "\n",
        "              # convert variables to floats for regression loss\n",
        "              labels = labels.type(torch.LongTensor).to(device)\n",
        "              images = images.type(torch.FloatTensor).to(device)\n",
        "\n",
        "              feature_map_1 = torch.empty(feature_map_shape)\n",
        "              feature_map_2 = torch.empty(feature_map_shape)\n",
        "              feature_map_3 = torch.empty(feature_map_shape)\n",
        "              feature_map_4 = torch.empty(feature_map_shape)\n",
        "              feature_map_5 = torch.empty(feature_map_shape)\n",
        "\n",
        "              view_num = 0\n",
        "\n",
        "              for seed_view in images:\n",
        "                view_num += 1\n",
        "                if view_num == 1:\n",
        "                  # ([8, 256, 8, 8]), 8 = Batch, (256, 8, 8) = Feature Map\n",
        "                  feature_map_1 = f1_net(seed_view)\n",
        "                elif view_num == 2:\n",
        "                  feature_map_2 = f2_net(seed_view)\n",
        "                elif view_num == 3:\n",
        "                  feature_map_3 = f3_net(seed_view)\n",
        "                elif view_num == 4:\n",
        "                  feature_map_4 = f4_net(seed_view)\n",
        "                elif view_num == 5:\n",
        "                  feature_map_5 = f5_net(seed_view)\n",
        "\n",
        "              # FUSION OF FEATURE MAPS BY MAX-POLLING ACROSS FEATURE MAPS\n",
        "              # Convert [8 x [5 x 256 x 8 x 8]] to [8 x [256 x 8 x 8]]\n",
        "              # 5 feature map from each view is max-pooled into 1 feature map  \n",
        "\n",
        "              # Create [8 x 256 x 8 x 8] matrix to store output of max-pooling operation, [256 x 8 x 8] = fused feature map of 1 seed\n",
        "              fused_feature_map = torch.empty((feature_map_1.shape[0], feature_map_1.shape[1], feature_map_1.shape[2], feature_map_1.shape[3]), dtype=torch.float)\n",
        "\n",
        "              # Max-pool across layer\n",
        "              for i in range(feature_map_1.shape[0]):\n",
        "                  for j in  range(feature_map_1.shape[1]):\n",
        "                    for k in  range(feature_map_1.shape[2]):\n",
        "                      for l in  range(feature_map_1.shape[3]):\n",
        "                        list = [feature_map_1[i][j][k][l], feature_map_2[i][j][k][l], feature_map_3[i][j][k][l]]\n",
        "                        fused_feature_map[i][j][k][l] = torch.IntTensor.item(max(list)) # Convert max(list) tensor to int and \n",
        "\n",
        "            # Flatten feature map of each seed and stack \n",
        "              flattened_fused_feature_map = []\n",
        "              for i in fused_feature_map:\n",
        "                flattened_fused_feature_map.append(i.flatten())\n",
        "\n",
        "              # Convert to tensor of tensor\n",
        "              feature_map_concat = torch.stack(flattened_fused_feature_map)\n",
        "\n",
        "              # forward pass to get outputs\n",
        "              output_labels = model(feature_map_concat)\n",
        "\n",
        "              # calculate the loss between predicted and target keypoints\n",
        "              loss = criterion(output_labels, labels)\n",
        "\n",
        "              # zero the parameter (weight) gradients  \n",
        "              optimizer_f1.zero_grad()\n",
        "              optimizer_f2.zero_grad()\n",
        "              optimizer_f3.zero_grad()\n",
        "              optimizer_f4.zero_grad()\n",
        "              optimizer_f5.zero_grad()\n",
        "              optimizer_main.zero_grad()\n",
        "\n",
        "              # backward + optimize only if in training phase\n",
        "              loss.backward()\n",
        "              \n",
        "              # update the weights\n",
        "              optimizer_f1.step()\n",
        "              optimizer_f2.step()\n",
        "              optimizer_f3.step()\n",
        "              optimizer_f4.step()\n",
        "              optimizer_f5.step()\n",
        "              optimizer_main.step()\n",
        "\n",
        "              # print loss statistics\n",
        "              running_loss += loss.item()\n",
        "\n",
        "      if validation_in_training:\n",
        "        val_running_loss = 0.0\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "        with torch.no_grad():\n",
        "          for images, labels in test_dataloader:\n",
        "              # wrap them in a torch Variable\n",
        "              images, labels = Variable(images), Variable(labels)\n",
        "\n",
        "              # torch.Size([8, 5, 64, 64, 3]) --> torch.Size([5, 8, 3, 64, 64])\n",
        "              images = images.view(images.shape[1], images.shape[0], images.shape[4], images.shape[2],images.shape[3])\n",
        "\n",
        "              # convert variables to floats for regression loss\n",
        "              labels = labels.type(torch.LongTensor).to(device)\n",
        "              images = images.type(torch.FloatTensor).to(device)\n",
        "              val_total += labels.size()[0]\n",
        "\n",
        "              feature_map_1 = torch.empty(feature_map_shape)\n",
        "              feature_map_2 = torch.empty(feature_map_shape)\n",
        "              feature_map_3 = torch.empty(feature_map_shape)\n",
        "              feature_map_4 = torch.empty(feature_map_shape)\n",
        "              feature_map_5 = torch.empty(feature_map_shape)\n",
        "\n",
        "              view_num = 0\n",
        "\n",
        "              for seed_view in images:\n",
        "                view_num += 1\n",
        "                if view_num == 1:\n",
        "                  # ([8, 256, 8, 8]), 8 = Batch, (256, 8, 8) = Feature Map\n",
        "                  feature_map_1 = f1_net(seed_view)\n",
        "                elif view_num == 2:\n",
        "                  feature_map_2 = f2_net(seed_view)\n",
        "                elif view_num == 3:\n",
        "                  feature_map_3 = f3_net(seed_view)\n",
        "                elif view_num == 4:\n",
        "                  feature_map_4 = f4_net(seed_view)\n",
        "                elif view_num == 5:\n",
        "                  feature_map_5 = f5_net(seed_view)\n",
        "\n",
        "              # FUSION OF FEATURE MAPS BY MAX-POLLING ACROSS FEATURE MAPS\n",
        "              # Convert [8 x [5 x 256 x 8 x 8]] to [8 x [256 x 8 x 8]]\n",
        "              # 5 feature map from each view is max-pooled into 1 feature map  \n",
        "\n",
        "              # Create [8 x 256 x 8 x 8] matrix to store output of max-pooling operation, [256 x 8 x 8] = fused feature map of 1 seed\n",
        "              fused_feature_map = torch.empty((feature_map_1.shape[0], feature_map_1.shape[1], feature_map_1.shape[2], feature_map_1.shape[3]), dtype=torch.float)\n",
        "\n",
        "              # Max-pool across layer\n",
        "              for i in range(feature_map_1.shape[0]):\n",
        "                  for j in  range(feature_map_1.shape[1]):\n",
        "                    for k in  range(feature_map_1.shape[2]):\n",
        "                      for l in  range(feature_map_1.shape[3]):\n",
        "                        list = [feature_map_1[i][j][k][l], feature_map_2[i][j][k][l], feature_map_3[i][j][k][l]]\n",
        "                        fused_feature_map[i][j][k][l] = torch.IntTensor.item(max(list)) # Convert max(list) tensor to int and \n",
        "\n",
        "            # Flatten feature map of each seed and stack \n",
        "              flattened_fused_feature_map = []\n",
        "              for i in fused_feature_map:\n",
        "                flattened_fused_feature_map.append(i.flatten())\n",
        "\n",
        "              # Convert to tensor of tensor\n",
        "              feature_map_concat = torch.stack(flattened_fused_feature_map)\n",
        "\n",
        "              # forward pass to get outputs\n",
        "              outputs = model(feature_map_concat)\n",
        "\n",
        "              _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "              val_loss = criterion(outputs, labels)\n",
        "\n",
        "              val_running_loss += val_loss.item()\n",
        "\n",
        "              val_correct += (predicted == labels).sum().item()\n",
        "\n",
        "        \n",
        "        writer.add_scalar('Validation Loss', val_running_loss / len(test_dataset), epoch)\n",
        "        writer.add_scalar('Accuracy', val_correct/val_total, epoch)\n",
        "\n",
        "      epoch_loss = running_loss / len(train_dataset)\n",
        "      print('Training Loss: {:.8f}'.format(epoch_loss))\n",
        "\n",
        "      writer.add_scalar('Training Loss', epoch_loss, epoch)\n",
        "      \n",
        "# torch.save({\n",
        "#             'model_state_dict': model.state_dict(),\n",
        "#             }, 'grayscale_self.model.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKY3WuIMXFAO"
      },
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NgWJCdKslKWS"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "\n",
        "total = 0  # keeps track of how many images we have processed \n",
        "correct = 0 # keeps track of how many correct images our self.model predicts\n",
        "true_pos_and_neg = 0\n",
        "true_pos = 0\n",
        "true_neg = 0\n",
        "false_pos = 0\n",
        "false_neg = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "      for images, labels in test_dataloader:\n",
        "\n",
        "        # wrap them in a torch Variable\n",
        "        images, labels = Variable(images), Variable(labels)\n",
        "\n",
        "        # torch.Size([8, 5, 64, 64, 3]) --> torch.Size([5, 8, 3, 64, 64])\n",
        "        images = images.view(images.shape[1], images.shape[0], images.shape[4], images.shape[2],images.shape[3])\n",
        "\n",
        "        # convert variables to floats for regression loss\n",
        "        labels = labels.type(torch.LongTensor).to(device)\n",
        "        images = images.type(torch.FloatTensor).to(device)\n",
        "\n",
        "        feature_map_1 = torch.empty(feature_map_shape)\n",
        "        feature_map_2 = torch.empty(feature_map_shape)\n",
        "        feature_map_3 = torch.empty(feature_map_shape)\n",
        "        feature_map_4 = torch.empty(feature_map_shape)\n",
        "        feature_map_5 = torch.empty(feature_map_shape)\n",
        "\n",
        "        view_num = 0\n",
        "\n",
        "        for seed_view in images:\n",
        "          view_num += 1\n",
        "          if view_num == 1:\n",
        "            # ([8, 256, 8, 8]), 8 = Batch, (256, 8, 8) = Feature Map\n",
        "            feature_map_1 = f1_net(seed_view)\n",
        "          elif view_num == 2:\n",
        "            feature_map_2 = f2_net(seed_view)\n",
        "          elif view_num == 3:\n",
        "            feature_map_3 = f3_net(seed_view)\n",
        "          elif view_num == 4:\n",
        "            feature_map_4 = f4_net(seed_view)\n",
        "          elif view_num == 5:\n",
        "            feature_map_5 = f5_net(seed_view)\n",
        "        \n",
        "        # FUSION OF FEATURE MAPS BY MAX-POLLING ACROSS FEATURE MAPS\n",
        "        # Convert [8 x [5 x 256 x 8 x 8]] to [8 x [256 x 8 x 8]]\n",
        "        # 5 feature map from each view is max-pooled into 1 feature map  \n",
        "\n",
        "        # Create [8 x 256 x 8 x 8] matrix to store output of max-pooling operation, [256 x 8 x 8] = fused feature map of 1 seed\n",
        "        fused_feature_map = torch.empty((feature_map_1.shape[0], feature_map_1.shape[1], feature_map_1.shape[2], feature_map_1.shape[3]), dtype=torch.float)\n",
        "\n",
        "        # Max-pool across layer\n",
        "        for i in range(feature_map_1.shape[0]):\n",
        "            for j in  range(feature_map_1.shape[1]):\n",
        "              for k in  range(feature_map_1.shape[2]):\n",
        "                for l in  range(feature_map_1.shape[3]):\n",
        "                  list = [feature_map_1[i][j][k][l], feature_map_2[i][j][k][l], feature_map_3[i][j][k][l]]\n",
        "                  fused_feature_map[i][j][k][l] = torch.IntTensor.item(max(list)) # Convert max(list) tensor to int and \n",
        "\n",
        "        # Flatten feature map of each seed and stack \n",
        "        flattened_fused_feature_map = []\n",
        "        for i in fused_feature_map:\n",
        "          flattened_fused_feature_map.append(i.flatten())\n",
        "\n",
        "          # Convert to tensor of tensor\n",
        "          feature_map_concat = torch.stack(flattened_fused_feature_map)\n",
        "\n",
        "        outputs = model(feature_map_concat)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size()[0]\n",
        "\n",
        "        true_pos_and_neg += sum(map(lambda x,y : x == y, labels, predicted)) \n",
        "        true_pos += sum(map(lambda x,y : x == y and x == 1, labels, predicted))\n",
        "        true_neg += sum(map(lambda x,y : x == y and x == 0, labels, predicted))\n",
        "        false_pos += sum(map(lambda x,y : x != y and x == 0, labels, predicted))\n",
        "        false_neg += sum(map(lambda x,y : x != y and x == 1, labels, predicted)) \n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "precision = true_pos / (true_pos + false_pos)\n",
        "recall = true_pos / (true_pos + false_neg)\n",
        "f1 = 2 * ((precision * recall) / (precision + recall))\n",
        "        \n",
        "print('\\n==============Results==============')\n",
        "print(\"Accuracy: \", correct/total)\n",
        "print(\"F1 score: \", f1.cpu().numpy())\n",
        "print('\\n')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Displaying Tensorboard Panel in Google Colab"
      ],
      "metadata": {
        "id": "w_s7HwTxwBN-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KmccPU57bCY1"
      },
      "outputs": [],
      "source": [
        "# %load_ext tensorboard\n",
        "# %tensorboard --logdir logs"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "RGB_New_MaxPool_Fusion.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.2 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.2"
    },
    "vscode": {
      "interpreter": {
        "hash": "42a6ee17ce332ac733f0b1e7b6b697817fb0878bd3f615dee20e5cd0b769cdff"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}